#!/bin/bash
#SBATCH --partition=pal


# assign roles
# sbatch should actually make sure that $HOSTNAME==$PAL_MASTER (ie the node running this script is first on the nodelist)
# export PAL_MASTER=`scontrol show hostname $SLURM_NODELIST | head -n 1 | awk '{ print $1 "-ib" }'`
export PAL_MASTER=`hostname`

mkdir -p tmp
export SPARK_SLAVES=tmp/slaves.$SLURM_JOB_ID
scontrol show hostname $SLURM_NODELIST | awk '{ print $1 "-ib" }' | tail -n+2 >$SPARK_SLAVES

# start master and slaves
bin/start-master.sh
bin/start-slaves.sh

# this command only needs to be run on one task since Spark handles the job launching
srun --nodes=1 --ntasks-per-node=1 ./run-example -Dspark.executor.memory=50g org.apache.spark.examples.SparkPageRank spark://$PAL_MASTER:7077 /pic/projects/grappa/twitter/twitter_1.4Bedge.tsv 10


